<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="pragma" content="no-cache">
  <meta http-equiv="cache-control" content="no-cache">
  <meta http-equiv="expires" content="0">
  
  <title>GiddyPoet</title>
  <meta name="author" content="GiddyPoet">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="GiddyPoet"/>

  
    <meta property="og:image" content=""/>
  

  
  
    <link href="/favicon.png" rel="icon">
  
  
  <link rel="stylesheet" href="/css/bootstrap.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>

  <!-- analytics -->
  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-70812759-1', 'auto');
  ga('send', 'pageview');
</script>



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?cb5448498d7169c668b07c2b255d62c1";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>


<meta name="generator" content="Hexo 6.0.0"></head>

 <body>  
  <nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
		<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
	  <a class="navbar-brand" href="/">GiddyPoet</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/archives" title="All the articles.">
			  <i class=""></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/categories" title="All the categories.">
			  <i class=""></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/tags" title="All the tags.">
			  <i class=""></i>Tags
			</a>
		  </li>
		  
		  <li>
			<a href="/about" title="About me.">
			  <i class=""></i>About
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
  	<div class="content">
    	 <div class="page-header logo">
  <h1>GiddyPoet<span class="blink-fast">∎</span></h1>
</div>

<div class="row page">

	
	<div class="col-md-9">
	

		<div class="slogan">


		<i class="fa fa-heart blink-slow"></i>

		Better to run than curse the road.

</div>    

		<div id="top_search"></div>
		<div class="mypage">
		
		<!-- title and entry -->
		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2022/02/08/linux网卡聚合及bond模式原理/" >linux网卡聚合及bond模式原理</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2022-02-08  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<h1 id="linux网卡聚合及bond模式原理"><a href="#linux网卡聚合及bond模式原理" class="headerlink" title="linux网卡聚合及bond模式原理"></a>linux网卡聚合及bond模式原理</h1><h2 id="bonding的应用"><a href="#bonding的应用" class="headerlink" title="bonding的应用"></a>bonding的应用</h2><h3 id="网络负载均衡"><a href="#网络负载均衡" class="headerlink" title="网络负载均衡"></a>网络负载均衡</h3><p>用于解决一个ip地址，流量过大，服务器网络压力过大的问题。将多个网卡聚合在一起生成一个bond口，将ip配在这个bond口上，具体mac地址根据bond口的模式选择。</p>
<h3 id="网络冗余"><a href="#网络冗余" class="headerlink" title="网络冗余"></a>网络冗余</h3><p>通过冗余服务提供服务器的可靠性和安全性，多个网卡绑定到一个IP地址上，当一个网卡发生物理损坏时，另一块网卡也能提供正常服务。</p>
<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>根据不同的模式有不同的原理，在介绍<a href="#%E6%A8%A1%E5%BC%8F">模式</a>时详细介绍。</p>
<p><img src="/2022/02/08/linux%E7%BD%91%E5%8D%A1%E8%81%9A%E5%90%88%E5%8F%8Abond%E6%A8%A1%E5%BC%8F%E5%8E%9F%E7%90%86/bond.jpg"></p>
<h2 id="模式"><a href="#模式" class="headerlink" title="模式"></a>模式</h2><p>linux有7种网卡绑定模式：</p>
<table>
<thead>
<tr>
<th align="center">模式代号</th>
<th align="center">模式名称</th>
<th align="center">模式方式</th>
<th align="center">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0</td>
<td align="center">balance-rr</td>
<td align="center">轮询模式</td>
<td align="center">按照设备顺序依次传输数据包，直到最后一个设备。提供负载均衡和容错能力</td>
</tr>
<tr>
<td align="center">1</td>
<td align="center">active-backup</td>
<td align="center">主备策略</td>
<td align="center">只有一个设备处于活动状态。一个宕机另一个马上由备变成主，mac地址外部可见。</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">balance-xor</td>
<td align="center">异或策略</td>
<td align="center">该策略是根据mac地址异或运算的结果来选择传输设备</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">broadcase policy</td>
<td align="center">广播策略</td>
<td align="center">该策略将所有数据包传输给所有接口通过全部设备来传输所有数据，提供容错能力。（每个网卡上发一份）</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">802.3ad</td>
<td align="center">动态链路聚合</td>
<td align="center">该策略通过创建聚合组来共享相同的传输能力，需要交换机支持802.3ad</td>
</tr>
<tr>
<td align="center">5</td>
<td align="center">balance-tlb</td>
<td align="center">适配器传输负载均衡</td>
<td align="center">发送数据给每一个设备，由当前试用的设备处理收到的数据。（这个不是业务的问题吗？和广播策略的异同）</td>
</tr>
<tr>
<td align="center">6</td>
<td align="center">balance-alb</td>
<td align="center">适配器负载均衡</td>
<td align="center">该策略在IPV4情况下包含适配器传输负载均衡策略，由ARP协商完成接收的负载，通道联合驱动程序截获ARP在本地系统发送出的请求，用其中一个设备的硬件地址覆盖从属设备的原地址。</td>
</tr>
</tbody></table>
<h3 id="balance-rr"><a href="#balance-rr" class="headerlink" title="balance-rr"></a>balance-rr</h3><h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><ul>
<li>负载均衡<ul>
<li>所有链路处于负载均衡状态，采用轮询的方式往每条链路发送报文，可以通过ping验证。</li>
</ul>
</li>
<li>容错问题<ul>
<li>增加了带宽，当链路错误后，会将流量切换到正常的链路上。（这个问题需要跟一下，为什么在rr模式下会丢包）</li>
</ul>
</li>
<li>性能问题<ul>
<li>连接包从不同的接口发出，中途经过不同的链路，在客户端处可能出现包乱序问题（也有可能在某种情况下，链路网络状态差导致丢包），在实际使用中避免接不同的交换机可以在一定程度上避免这个问题。</li>
</ul>
</li>
<li>交换机支持<ul>
<li>该模式下所有绑定的网卡的ip都被修改成同一个mac地址，此时交换机收到发往该mac地址的数据包，将不知道从对应的那个端口发该数据，交换机应该做端口绑定，将数据发往给逻辑端口，之后由逻辑端口转发数据。</li>
</ul>
</li>
<li>原理<ul>
<li>实际上就是将所有网卡的地址变为一个地址，然后由交换机将报文发送给bond口，然后由驱动层算法分别转发到不同的真实网卡</li>
</ul>
</li>
</ul>
<p><img src="/2022/02/08/linux%E7%BD%91%E5%8D%A1%E8%81%9A%E5%90%88%E5%8F%8Abond%E6%A8%A1%E5%BC%8F%E5%8E%9F%E7%90%86/bond_rr.png"></p>
<h3 id="active-backup"><a href="#active-backup" class="headerlink" title="active-backup"></a>active-backup</h3><h4 id="特点-1"><a href="#特点-1" class="headerlink" title="特点"></a>特点</h4><ul>
<li>容错能力<ul>
<li>只有一个slave是激活状态，同时只有一个网卡能够工作，只有当当前网卡故障后其他网卡才会激活。发生failover时，切换的网卡会发出arp请求</li>
</ul>
</li>
<li>无负载均模式<ul>
<li>同时只有一个接口处于工作状态</li>
</ul>
</li>
<li>无需交换机支持<ul>
<li>mac地址采用的是bond0的mac地址，因此mac地址是唯一的（这个为什么会有只有一个mac地址，如果交换机多个端口都是这个mac地址没有问题吗？）</li>
<li>实际上mac地址都是一致的，但是只有一个网卡生效</li>
</ul>
</li>
</ul>
<p><img src="/2022/02/08/linux%E7%BD%91%E5%8D%A1%E8%81%9A%E5%90%88%E5%8F%8Abond%E6%A8%A1%E5%BC%8F%E5%8E%9F%E7%90%86/bond_ac.png"></p>
<p><img src="/2022/02/08/linux%E7%BD%91%E5%8D%A1%E8%81%9A%E5%90%88%E5%8F%8Abond%E6%A8%A1%E5%BC%8F%E5%8E%9F%E7%90%86/bond_ac_log.png"></p>
<h3 id="load-balancing-xor"><a href="#load-balancing-xor" class="headerlink" title="load balancing(xor)"></a>load balancing(xor)</h3><h4 id="特点-2"><a href="#特点-2" class="headerlink" title="特点"></a>特点</h4><ul>
<li>负载均衡和容错能力<ul>
<li>基于指定的传输HASH策略传输数据包。缺省的负载均衡策略是：(源mac地址XOR目标mac地址)%slave数量，其他传输策略可以通过xmit_hash_policy选项指定。</li>
</ul>
</li>
<li>性能问题<ul>
<li>该模式将限定流量，以保证到达特定对端的流量总是从同一个接口上发出。既然目的地是通过MAC地址来决定的，因此该模式在“本地”网络配置下可以工作得很好。如果所有流量是通过单个路由器（比如 “网关”型网络配置，只有一个网关时，源和目标mac都固定了，那么这个算法算出的线路就一直是同一条，那么这种模式就没有多少意义了。）源mac是交换机的mac，目的mac是bond0上的mac。再交换机后不建议采用该模式</li>
</ul>
</li>
<li>交换机支持<ul>
<li>和balance-rr一样，需要交换机配置成“port channel”。该模式通过源和目标mac做hash因子来做xor算法来选路。</li>
</ul>
</li>
</ul>
<h3 id="fault-tolerance"><a href="#fault-tolerance" class="headerlink" title="fault-tolerance"></a>fault-tolerance</h3><p>这种模式的特点是一个报文会复制两份往bond下的两个接口分别发送出去，当有对端交换机失效，我们感觉不到任何downtime，但此法过于浪费资源；不过这种模式有很好的容错机制。此模式适用于金融行业，因为他们需要高可靠性的网络，不允许出现任何问题。</p>
<h4 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h4><p>这种模式适用于如下拓扑，两个接口分别接入两台交换机，并且属于不同的vlan，当一边的网络出现故障不会影响服务器另一边接入的网络正常工作。而且故障过程是0丢包。</p>
<h3 id="lacp-802-3ad"><a href="#lacp-802-3ad" class="headerlink" title="lacp(802.3ad)"></a>lacp(802.3ad)</h3><h4 id="特点-3"><a href="#特点-3" class="headerlink" title="特点"></a>特点</h4><p>802.3ad模式是IEEE标准，因此所有实现了802.3ad的对端都可以很好的互操作。802.3ad 协议包括聚合的自动配置，因此只需要很少的对交换机的手动配置（要指出的是，只有某些设备才能使用802.3ad）。802.3ad标准也要求帧按顺序（一定程度上）传递，因此通常单个连接不会看到包的乱序。802.3ad也有些缺点：标准要求所有设备在聚合操作时，要在同样的速率和双工模式，而且，和除了balance-rr模式外的其它bonding负载均衡模式一样，任何连接都不能使用多于一个接口的带宽。（保证了连接的顺序传递，但是所有接口的速率同步）</p>
<p>要求参与绑定的端口都运行802.3ad协议。该方式与band0很相似，但是又有区别。在IEEE802.3ad中，“链路聚合控制协议（LACP）”自动通知交换机应该聚合哪些端口，IEEE802.3ad聚合配置之后，链路聚合控制协议单元（LACPDU）就会在服务器和交换机之间进行交换，LACP会通知交换机在聚合中配置的适配器应作为交换机上的一个适配器来考虑，而不再有用户的干预。（按照协议应当如此，但是H3C5500-EI交换机上并未发现有单独启用802.3ad或LACP的命令，而聚合组中的静态聚合的所有端口均不启用LACP协议，所以如果按照band4的模式操作的话，建议在交换机上手工做动态端口聚合，且手工指定全局报文按照源MAC和目的MAC地址进行聚合负载分担。）根据IEEE802.3ad的规范，前往相同IP地址的数据包都会通过相同的适配器进行发送。因此当在802.3ad方式下操作时，信息包会始终按照标准（standard）的方式进行分发，而不会按照轮询（Round-Robin）方式进行分发。</p>
<ul>
<li>交换机配置<ul>
<li>interface AggregatePort 1 配置聚合口 interface GigabitEthernet 0&#x2F;23 port-group 1 mode active 接口下开启lacp 主动模式 interface GigabitEthernet 0&#x2F;24 port-group 1 mode active</li>
</ul>
</li>
<li>必要条件<ul>
<li>ethtool支持获取每个slave的速率和双工设定 </li>
<li>switch(交换机)支持IEEE 802.3ad Dynamic link aggregation </li>
<li>大多数switch(交换机)需要经过特定配置才能支持802.3ad模式</li>
</ul>
</li>
</ul>
<h3 id="balance-tlb"><a href="#balance-tlb" class="headerlink" title="balance-tlb"></a>balance-tlb</h3><p>balance-tlb模式通过对端均衡外出（outgoing）流量。既然它是根据MAC地址进行均衡，在“网关”型配置（如上文所述）下，该模式会通过单个设备来发送所有流量，然而，在“本地”型网络配置下，该模式以相对智能的方式（不是balance-xor或802.3ad模式里提及的XOR方式）来均衡多个本地网络对端，因此那些数字不幸的MAC地址（比如XOR得到同样值）不会聚集到同一个接口上。不像802.3ad，该模式的接口可以有不同的速率，而且不需要特别的交换机配置。不利的一面在于，该模式下所有进入的（incoming）流量会到达同一个接口；该模式要求slave接口的网络设备驱动有某种ethtool支持；而且ARP监控不可用。</p>
<h3 id="balance-alb"><a href="#balance-alb" class="headerlink" title="balance-alb"></a>balance-alb</h3><p>该模式包含了balance-tlb模式，同时加上针对IPV4流量的接收负载均衡(receive load balance， rlb)，而且不需要任何switch(交换机)的支持。接收负载均衡是通过ARP协商实现的。bonding驱动截获本机发送的ARP应答，并把源硬件地址改写为bond中某个slave的唯一硬件地址，从而使得不同的对端使用不同的硬件地址进行通信。所有端口都会收到对端的arp请求报文，回复arp回时，bond驱动模块会截获所发的arp回复报文，根据算法算到相应端口，这时会把arp回复报文的源mac，send源mac都改成相应端口mac。从抓包情况分析回复报文是第一个从端口1发，第二个从端口2发。以此类推。</p>
<p>当本机发送ARP请求时，bonding驱动把对端的IP信息从ARP包中复制并保存下来。当ARP应答从对端到达时，bonding驱动把它的硬件地址提取出来，并发起一个ARP应答给bond中的某个slave(这个算法和上面一样，比如算到1口，就给发送arp请求，1回复时mac用1的mac)。使用ARP协商进行负载均衡的一个问题是：每次广播 ARP请求时都会使用bond的硬件地址，因此对端学习到这个硬件地址后，接收流量将会全部流向当前的slave。这个问题通过给所有的对端发送更新（ARP应答）来解决，往所有端口发送应答，应答中包含他们独一无二的硬件地址，从而导致流量重新分布。当新的slave加入到bond中时，或者某个未激活的slave重新激活时，接收流量也要重新分布。接收的负载被顺序地分布（round robin）在bond中最高速的slave上。</p>

	
	</div>
  <a type="button" href="/2022/02/08/linux网卡聚合及bond模式原理/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2022/02/03/一次内存异常问题排查经历/" >一次内存异常问题排查经历</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2022-02-03  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<h2 id="内存异常现象"><a href="#内存异常现象" class="headerlink" title="内存异常现象"></a>内存异常现象</h2><p>在客户现场运行的环境发现服务器内存持续性的增加，设备本身有64g的内存，最终服务器使用了将近80%的内存。</p>
<h2 id="排查思路"><a href="#排查思路" class="headerlink" title="排查思路"></a>排查思路</h2><h3 id="使用free-mh查看系统内存占用情况"><a href="#使用free-mh查看系统内存占用情况" class="headerlink" title="使用free -mh查看系统内存占用情况"></a>使用<code>free -mh</code>查看系统内存占用情况</h3><p>发现buff&#x2F;cache占用大量的内存</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 此示例只代表free -mh的结果和实际情况不符，只是记忆中大致数据，显示的就是buff/cache占用大量的内存</span></span><br><span class="line">[root@TAC ~]# free -mh</span><br><span class="line">              total        used        free      shared  buff/cache   available</span><br><span class="line">Mem:            62G        6.8G         8G         80M        49G         8G</span><br><span class="line">Swap:           31G          0B         31G</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>大致介绍一下<code>free</code>命令输出的含义</p>
<ul>
<li>mem 内存使用情况<ul>
<li>total 总共</li>
<li>used 已用</li>
<li>free 空闲</li>
<li>shared 共享使用 tmpfs所用的内存</li>
<li>buff&#x2F;cache <ul>
<li>buff 内核缓存区所用内存</li>
<li>cache 页面缓存和slab使用的内存（kmalloc）</li>
</ul>
</li>
<li>available 预估启动新进程可用内存，不包含交换空间</li>
</ul>
</li>
<li>swap 交换空间使用情况<ul>
<li>total</li>
<li>used</li>
<li>free</li>
</ul>
</li>
</ul>
<h4 id="swap"><a href="#swap" class="headerlink" title="swap"></a>swap</h4><p>swap space 是磁盘上的一块区域，可以是一个分区，也可以是一个文件。所以具体的实现可以是 swap 分区也可以是 swap 文件。当系统物理内存吃紧时，Linux 会将内存中不常访问的数据保存到 swap 上，这样系统就有更多的物理内存为各个进程服务，而当系统需要访问 swap 上存储的内容时，再将 swap 上的数据加载到内存中，这就是常说的换出和换入。交换空间可以在一定程度上缓解内存不足的情况，但是它需要读写磁盘数据，所以性能不是很高。</p>
<p>现在的机器一般都不太缺内存，如果系统默认还是使用了 swap 是不是会拖累系统的性能？理论上是的，但实际上可能性并不是很大。并且内核提供了一个叫做 swappiness 的参数，用于配置需要将内存中不常用的数据移到 swap 中去的紧迫程度。这个参数的取值范围是 0～100，0 告诉内核尽可能的不要将内存数据移到 swap 中，也即只有在迫不得已的情况下才这么做，而 100 告诉内核只要有可能，尽量的将内存中不常访问的数据移到 swap 中。</p>
<p>内核参数位置<code>/proc/sys/vm/swappiness</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@TAC ~]# cat /proc/sys/vm/swappiness </span><br><span class="line">60</span><br></pre></td></tr></table></figure>

<h4 id="shared"><a href="#shared" class="headerlink" title="shared"></a>shared</h4><p>主要是只tmpfs系统所占用的内存，该文件系统是挂载到内存当中的</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 如下tmpfs所占用的的空间，该空间在重启后数据将丢失，内存文件系统</span></span><br><span class="line">[root@TAC tmpfiles.d]# df -h</span><br><span class="line">Filesystem      Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/sda4       3.6T   84G  3.3T   3% /</span><br><span class="line">devtmpfs         32G     0   32G   0% /dev</span><br><span class="line">tmpfs            32G   28K   32G   1% /dev/shm</span><br><span class="line">tmpfs            32G   58M   32G   1% /run</span><br><span class="line">tmpfs            32G     0   32G   0% /sys/fs/cgroup</span><br><span class="line">/dev/sda2       976M  145M  765M  16% /boot</span><br><span class="line">tmpfs           6.3G     0  6.3G   0% /run/user/0</span><br></pre></td></tr></table></figure>
<p>内核会动态分配tmpfs的大小</p>
<h4 id="buff-x2F-cache"><a href="#buff-x2F-cache" class="headerlink" title="buff&#x2F;cache"></a>buff&#x2F;cache</h4><p>buffer和cache是两个在计算机技术中被用滥的名词，放在不通语境下会有不同的意义。在Linux的内存管理中，这里的buffer指Linux内存的：Buffer cache。这里的cache指Linux内存中的：Page cache。翻译成中文可以叫做缓冲区缓存和页面缓存。在历史上，它们一个（buffer）被用来当成对io设备写的缓存，而另一个（cache）被用来当作对io设备的读缓存，这里的io设备，主要指的是块设备文件和文件系统上的普通文件。但是现在，它们的意义已经不一样了。在当前的内核中，page cache顾名思义就是针对内存页的缓存，说白了就是，如果有内存是以page进行分配管理的，都可以使用page cache作为其缓存来管理使用。当然，不是所有的内存都是以页（page）进行管理的，也有很多是针对块（block）进行管理的，这部分内存使用如果要用到cache功能，则都集中到buffer cache中来使用。（从这个角度出发，是不是buffer cache改名叫做block cache更好？）然而，也不是所有块（block）都有固定长度，系统上块的长度主要是根据所使用的块设备决定的，而页长度在X86上无论是32位还是64位都是4k。</p>
<ul>
<li>buff: block buff针对块（block）进行管理的，这部分内存使用如果要用到cache功能，则都集中到buffer cache中来使用。（不正确的理解，用来读）</li>
<li>cache: page cache顾名思义就是针对内存页的缓存，说白了就是，如果有内存是以page进行分配管理的，都可以使用page cache作为其缓存来管理使用。（不正确的理解，用来写）</li>
</ul>
<h3 id="查看-proc-sys-meminfo"><a href="#查看-proc-sys-meminfo" class="headerlink" title="查看/proc/sys/meminfo"></a>查看<code>/proc/sys/meminfo</code></h3><p>发现buff&#x2F;cache占用大量内存，则就应该跟进查看具体buff&#x2F;cache哪个部分占用更为大量的内存<br>根据meminfo发现大量slab占用内存，将近42个g</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">[root@TAC tmpfiles.d]# cat /proc/meminfo </span><br><span class="line">MemTotal:       65430552 kB</span><br><span class="line">MemFree:        56802616 kB</span><br><span class="line">MemAvailable:   57562116 kB</span><br><span class="line">Buffers:          259960 kB</span><br><span class="line">Cached:           978228 kB</span><br><span class="line">SwapCached:            0 kB</span><br><span class="line">Active:          7181820 kB</span><br><span class="line">Inactive:         501588 kB</span><br><span class="line">Active(anon):    6445436 kB</span><br><span class="line">Inactive(anon):    98936 kB</span><br><span class="line">Active(file):     736384 kB</span><br><span class="line">Inactive(file):   402652 kB</span><br><span class="line">Unevictable:           0 kB</span><br><span class="line">Mlocked:               0 kB</span><br><span class="line">SwapTotal:      32834556 kB</span><br><span class="line">SwapFree:       32834556 kB</span><br><span class="line">Dirty:               288 kB</span><br><span class="line">Writeback:             0 kB</span><br><span class="line">AnonPages:       6445280 kB</span><br><span class="line">Mapped:           220768 kB</span><br><span class="line">Shmem:             99160 kB</span><br><span class="line">Slab:             244800 kB</span><br><span class="line">SReclaimable:     143828 kB</span><br><span class="line">SUnreclaim:       100972 kB</span><br><span class="line">KernelStack:       23776 kB</span><br><span class="line">PageTables:        56356 kB</span><br><span class="line">NFS_Unstable:          0 kB</span><br><span class="line">Bounce:                0 kB</span><br><span class="line">WritebackTmp:          0 kB</span><br><span class="line">CommitLimit:    65549832 kB</span><br><span class="line">Committed_AS:   47565480 kB</span><br><span class="line">VmallocTotal:   34359738367 kB</span><br><span class="line">VmallocUsed:      469040 kB</span><br><span class="line">VmallocChunk:   34358892540 kB</span><br><span class="line">HardwareCorrupted:     0 kB</span><br><span class="line">AnonHugePages:   5654528 kB</span><br><span class="line">HugePages_Total:       0</span><br><span class="line">HugePages_Free:        0</span><br><span class="line">HugePages_Rsvd:        0</span><br><span class="line">HugePages_Surp:        0</span><br><span class="line">Hugepagesize:       2048 kB</span><br><span class="line">DirectMap4k:      257584 kB</span><br><span class="line">DirectMap2M:     7776256 kB</span><br><span class="line">DirectMap1G:    60817408 kB</span><br></pre></td></tr></table></figure>

<ul>
<li>MemTotal: 所有内存(RAM)大小,减去一些预留空间和内核的大小。</li>
<li>MemFree: 完全没有用到的物理内存，lowFree+highFree</li>
<li>MemAvailable: 在不使用交换空间的情况下，启动一个新的应用最大可用内存的大小，计算方式：MemFree+Active(file)+Inactive(file)-(watermark+min(watermark,Active(file)+Inactive(file)&#x2F;2))</li>
<li>Buffers: 块设备所占用的缓存页，包括：直接读写块设备以及文件系统元数据(metadata)，比如superblock使用的缓存页。</li>
<li>Cached: 表示普通文件数据所占用的缓存页。</li>
<li>SwapCached: swap cache中包含的是被确定要swapping换页，但是尚未写入物理交换区的匿名内存页。那些匿名内存页，比如用户进程malloc申请的内存页是没有关联任何文件的，如果发生swapping换页，这类内存会被写入到交换区。</li>
<li>Active: active包含active anon和active file</li>
<li>Inactive: inactive包含inactive anon和inactive file</li>
<li>Active(anon): anonymous pages（匿名页），用户进程的内存页分为两种：与文件关联的内存页(比如程序文件,数据文件对应的内存页)和与内存无关的内存页（比如进程的堆栈，用malloc申请的内存），前者称为file pages或mapped pages,后者称为匿名页。</li>
<li>Inactive(anon): 见上</li>
<li>Active(file): 见上</li>
<li>Inactive(file): 见上</li>
<li>SwapTotal: 可用的swap空间的总的大小(swap分区在物理内存不够的情况下，把硬盘空间的一部分释放出来，以供当前程序使用)</li>
<li>SwapFree: 当前剩余的swap的大小</li>
<li>Dirty: 需要写入磁盘的内存去的大小</li>
<li>Writeback: 正在被写回的内存区的大小</li>
<li>AnonPages: 未映射页的内存的大小</li>
<li>Mapped: 设备和文件等映射的大小</li>
<li>Slab: 内核数据结构slab的大小</li>
<li>SReclaimable: 可回收的slab的大小</li>
<li>SUnreclaim: 不可回收的slab的大小</li>
<li>PageTables: 管理内存页页面的大小</li>
<li>NFS_Unstable: 不稳定页表的大小</li>
<li>VmallocTotal: Vmalloc内存区的大小</li>
<li>VmallocUsed: 已用Vmalloc内存区的大小</li>
<li>VmallocChunk: vmalloc区可用的连续最大快的大小</li>
</ul>
<h3 id="slabtop查看为什么slab占用大量内存"><a href="#slabtop查看为什么slab占用大量内存" class="headerlink" title="slabtop查看为什么slab占用大量内存"></a>slabtop查看为什么slab占用大量内存</h3><p>根据slabtop现场发现，大量kmalloc-2048占用大量内存，大概率是由于内核新加的一个驱动程序异常导致内存被大量占用，卸载后恢复正常</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">[root@TAC tmpfiles.d]# slabtop </span><br><span class="line">Active / Total Objects (% used)    : 972765 / 979478 (99.3%)</span><br><span class="line">Active / Total Slabs (% used)      : 21676 / 21676 (100.0%)</span><br><span class="line">Active / Total Caches (% used)     : 77 / 110 (70.0%)</span><br><span class="line">Active / Total Size (% used)       : 238632.48K / 242106.66K (98.6%)</span><br><span class="line">Minimum / Average / Maximum Object : 0.01K / 0.25K / 12.62K</span><br><span class="line"></span><br><span class="line">OBJS ACTIVE  USE OBJ SIZE  SLABS OBJ/SLAB CACHE SIZE NAME                   </span><br><span class="line">224532 224197  99%    0.19K   5346	 42     42768K dentry</span><br><span class="line">159393 159393 100%    0.10K   4087	 39     16348K buffer_head</span><br><span class="line">65052  65052 100%    0.11K   1807	 36	 7228K kernfs_node_cache</span><br><span class="line">58432  56763  97%    0.06K    913	 64	 3652K kmalloc-64</span><br><span class="line">51119  51119 100%    1.01K   1649	 31     52768K ext4_inode_cache</span><br><span class="line">42587  42587 100%    0.21K   1151	 37	 9208K vm_area_struct</span><br><span class="line">32946  32946 100%    0.04K    323	102	 1292K ext4_extent_status</span><br><span class="line">32538  32538 100%    0.04K    319	102	 1276K selinux_inode_security</span><br><span class="line">29580  29580 100%    0.13K    493	 60	 3944K ext4_groupinfo_4k</span><br><span class="line">26880  26880 100%    0.19K    640	 42	 5120K kmalloc-192</span><br><span class="line">24320  23098  94%    0.25K    380	 64	 6080K kmalloc-256</span><br><span class="line">23856  23856 100%    0.07K    426	 56	 1704K Acpi-ParseExt</span><br><span class="line">21930  21291  97%    0.08K    430	 51	 1720K anon_vma</span><br><span class="line">21824  21824 100%    0.50K    341	 64     10912K kmalloc-512</span><br><span class="line">17920  17920 100%    0.01K     35	512	  140K kmalloc-8</span><br><span class="line">16640  16640 100%    0.02K     65	256	  260K kmalloc-16</span><br><span class="line">14847  14528  97%    0.64K    303	 49	 9696K proc_inode_cache</span><br><span class="line">13200  11762  89%    0.58K    240	 55	 7680K inode_cache</span><br><span class="line">9912   9912 100%    0.57K    177	 56	 5664K radix_tree_node</span><br><span class="line">9088   9088 100%    0.03K     71	128	  284K kmalloc-32</span><br><span class="line">8896   8708  97%    1.00K    278	 32	 8896K kmalloc-1024</span><br><span class="line">7308   7308 100%    0.09K    174	 42	  696K kmalloc-96</span><br><span class="line">5952   5952 100%    0.12K     93	 64	  744K kmalloc-128</span><br><span class="line">4672   4672 100%    0.06K     73	 64	  292K ext4_free_data</span><br><span class="line">4590   4590 100%    0.05K     54	 85	  216K shared_policy_node</span><br><span class="line">3468   3468 100%    0.62K     68	 51	 2176K sock_inode_cache</span><br></pre></td></tr></table></figure>

	
	</div>
  <a type="button" href="/2022/02/03/一次内存异常问题排查经历/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2022/02/03/dpdk安装部署/" >dpdk安装部署</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2022-02-03  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<h1 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h1><p>DPDK在安装时需要对操作系统进行相应的驱动、内存等等做相应的设置，以提升</p>
<h2 id="UMA架构"><a href="#UMA架构" class="headerlink" title="UMA架构"></a>UMA架构</h2><p>在一开始，内存控制器还在北桥中，所有CPU对内存的访问都要通过北桥来完成。此时所有CPU访问内存都是“一致的”，如下图所示：</p>
<p><img src="/2022/02/03/dpdk%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/uma.webp"></p>
<p>这样的架构称为UMA(Uniform Memory Access)，直译为“统一内存访问”，这样的架构对软件层面来说非常容易，总线模型保证所有的内存访问是一致的，即每个处理器核心共享相同的内存地址空间。但随着CPU核心数的增加，这样的架构难免遇到问题，比如对总线的带宽带来挑战、访问同一块内存的冲突问题。为了解决这些问题，有人搞出了NUMA。</p>
<h2 id="NUMA架构"><a href="#NUMA架构" class="headerlink" title="NUMA架构"></a>NUMA架构</h2><p>NUMA 全称 Non-Uniform Memory Access，译为“非一致性内存访问”。这种构架下，不同的内存器件和CPU核心从属不同的 Node，每个 Node 都有自己的集成内存控制器（IMC，Integrated Memory Controller）。</p>
<p><img src="/2022/02/03/dpdk%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/numa.webp"></p>
<p>在上述架构中，通常一个内存插槽对应一个Node。需要注意的一个特点是，QPI的延迟要高于IMC Bus，也就是说CPU访问内存有了远近（remote&#x2F;local）之别，而且实验分析来看，这个差别非常明显。</p>
<h2 id="查看架构"><a href="#查看架构" class="headerlink" title="查看架构"></a>查看架构</h2><p>可以通过numactl查看或者通过查看cpu设备信息。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@GiddyPoet ~]# numactl --hardware</span><br><span class="line">available: 1 nodes (0)</span><br><span class="line">node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19</span><br><span class="line">node 0 size: 65189 MB</span><br><span class="line">node 0 free: 56946 MB</span><br><span class="line">node distances:</span><br><span class="line">node   0 </span><br><span class="line">  0:  10 </span><br><span class="line"></span><br><span class="line">[root@GiddyPoet node]# ls /sys/devices/system/node/</span><br><span class="line">has_cpu  has_memory  has_normal_memory  node0  online  possible  power  uevent</span><br></pre></td></tr></table></figure>

<p>通过上述信息可以查看cpu架构，上述都是uma架构，目前我还没有发现numa架构。</p>
<h2 id="pps算法（包转发率）"><a href="#pps算法（包转发率）" class="headerlink" title="pps算法（包转发率）"></a>pps算法（包转发率）</h2><p>pps: package per sesond</p>
<p><img src="/2022/02/03/dpdk%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/%E5%B8%A7%E7%BB%93%E6%9E%84.webp"></p>
<p>以太帧最小载荷为46Bytes，但是算上帧首部和校验码，总共有64Bytes，但是在实际传输过程中，帧之间有12个Bytes的帧间距，每个帧前面还有7个字节的前导帧和1个字节的帧首界定符。</p>
<p>一个最短以太帧实际长度为：<br>$(12+7+1+6+6+2+46+4)*8&#x3D;672bit$</p>
<p>通常按照万兆光计算下64个字节的包转发率。</p>
<p>$10*1000&#x2F;672≈14.88Mpps$</p>
<h2 id="UIO-用户空间IO"><a href="#UIO-用户空间IO" class="headerlink" title="UIO:用户空间IO"></a>UIO:用户空间IO</h2><p>小的内核模块，用于将设备内存映射到用户空间，并注册中断。<br>uio_pci_generic 为linux 内核模块，提供此功能，可以通过 modprobe uio_pci_generic 加载。<br>但是其不支持虚拟功能，DPDK提供一个替代模块igb_uio模块。</p>
<h2 id="VFIO-后续补充"><a href="#VFIO-后续补充" class="headerlink" title="VFIO(后续补充)"></a>VFIO(后续补充)</h2><p>使用vfio不仅需要驱动支持，内核和bios都要支持，并配置IO虚拟化（如Intel VT-d)</p>
<h2 id="大页内存"><a href="#大页内存" class="headerlink" title="大页内存"></a>大页内存</h2><p>通过grub进行修改</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 修改/etc/default/grub，在GRUB_CMDLINE_LINUX后新增大页内存配置，同时支持iommu，对于iommu</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需要再虚拟机cpu里也做相应的修改</span></span><br><span class="line">default_hugepagesz=2m hugepagesz=2m hugepages=256 iommu=pt intel_iommu=on</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改后重新生成grub文件</span></span><br><span class="line">grub2-mkconfig -o /boot/grub2/grub.cfg</span><br></pre></td></tr></table></figure>

<p>修改&#x2F;etc&#x2F;fstab开机挂载打页内存</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nodev /mnt/huge hugetlbfs defaults 0 0</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="虚拟机环境配置"><a href="#虚拟机环境配置" class="headerlink" title="虚拟机环境配置"></a>虚拟机环境配置</h1><p>由于dpdk需要对cpu绑定做相应的设置，cpu核数最好是大于2核，这里我们采用4核，同时创建4个网卡用于dpdk测试（此处建议网卡数和cpu数对应，便于测试cpu绑定功能）</p>
<p><img src="/2022/02/03/dpdk%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/vmware%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%85%8D%E7%BD%AE.jpg"></p>
<blockquote>
<p>注：其实部分在虚拟配置中修改.vmx文件，将网卡从e1000修改为其他dpdk能识别的网卡类型，也可以通过dpdk中igb_uio驱动关闭检测网卡类型来实现，实际上只是dpdk无法识别该网卡类型，但是dpdk支持在此网卡上工作。</p>
</blockquote>
<pre><code>修改igb_uio驱动检测方法：
</code></pre>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> RTE_INTR_MODE_LEGACY:</span><br><span class="line">          <span class="comment">// 添加true，保证能通过检测</span></span><br><span class="line">          <span class="keyword">if</span> (pci_intx_mask_supported(udev-&gt;pdev)||<span class="literal">true</span>) &#123;                                             </span><br><span class="line">              dev_dbg(&amp;udev-&gt;pdev-&gt;dev, <span class="string">&quot;using INTX&quot;</span>);</span><br><span class="line">              udev-&gt;info.irq_flags = IRQF_SHARED | IRQF_NO_THREAD;</span><br><span class="line">              udev-&gt;info.irq = udev-&gt;pdev-&gt;irq;</span><br><span class="line">              udev-&gt;mode = RTE_INTR_MODE_LEGACY;</span><br><span class="line">              <span class="keyword">break</span>;</span><br><span class="line">          &#125;   </span><br></pre></td></tr></table></figure>

<h1 id="编译安装dpdk"><a href="#编译安装dpdk" class="headerlink" title="编译安装dpdk"></a>编译安装dpdk</h1><p>dpdk的版本选择很重要，由于centos通常是趋于稳定版的版本，所以编译器等环境都是较为老旧的版本，根据官方的相关建议，选用dpdk-18.11.11-stable版本做为测试。</p>
<h2 id="设置环境变量"><a href="#设置环境变量" class="headerlink" title="设置环境变量"></a>设置环境变量</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export RTE_SDK=/root/dpdk-stable-18.11.11/</span><br><span class="line">export RTE_TARGET=x86_64-native-linuxapp-gcc</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 如果需要对相关代码进行调试可以添加</span></span><br><span class="line">export RTE_CFLAGS=&quot;-O0 -g&quot;</span><br></pre></td></tr></table></figure>

<h2 id="编译dpdk"><a href="#编译dpdk" class="headerlink" title="编译dpdk"></a>编译dpdk</h2><p>Makefile层层嵌套，入口为<code>GNUmakefile</code>，指定平台为<code>x86_64-native-linuxapp-gcc</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在dpdk-stable-18.11.11/config中实现</span></span><br><span class="line">make install T=x86_64-native-linuxapp-gcc</span><br></pre></td></tr></table></figure>

<p>编译后的目录结构</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@giddypoet x86_64-native-linuxapp-gcc]# tree -aL 1</span><br><span class="line">.</span><br><span class="line">├── app  # 测试用二进制文件</span><br><span class="line">├── build #</span><br><span class="line">├── .config # config文件</span><br><span class="line">├── .config.orig</span><br><span class="line">├── include # 头文件</span><br><span class="line">├── kmod # 驱动文件</span><br><span class="line">├── lib # 库文件</span><br><span class="line">└── Makefile</span><br></pre></td></tr></table></figure>

<h2 id="配置dpdk"><a href="#配置dpdk" class="headerlink" title="配置dpdk"></a>配置dpdk</h2><h3 id="uio驱动"><a href="#uio驱动" class="headerlink" title="uio驱动"></a>uio驱动</h3><p>内核自带了<code>uio_pci_generic</code>驱动可以实现uio功能，dpdk提供了一个<code>igb_uio</code>模块，对于不支持传统中断的设置，例如虚拟功能设备，必须使用<code>igb_uio</code>来替代<code>uio_pci_generi</code>模块。</p>
<h2 id="常见问题解决"><a href="#常见问题解决" class="headerlink" title="常见问题解决"></a>常见问题解决</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">安装时可能会存在缺少`numa.h`，可以通过yum安装相关依赖。</span></span><br><span class="line">yum install numactl-devel</span><br></pre></td></tr></table></figure>

	
	</div>
  <a type="button" href="/2022/02/03/dpdk安装部署/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2022/02/03/虚拟网络设备/" >虚拟网络设备</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2022-02-03  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<h1 id="虚拟网络设备"><a href="#虚拟网络设备" class="headerlink" title="虚拟网络设备"></a>虚拟网络设备</h1><p>本文将从基础的虚拟网络设备进行介绍，开启linux网络虚拟化的学习之路。</p>
<h2 id="tun-x2F-tap"><a href="#tun-x2F-tap" class="headerlink" title="tun&#x2F;tap"></a>tun&#x2F;tap</h2><p>tap&#x2F;tun是linux内核实现的一对虚拟网络设备，TAP工作在二层，tun工作在三层，linux内核通过TAP&#x2F;TUN设备向绑定该设备的用户空间应用发送数据。反之，用户空间也可以像操作网络硬件设备那样，通过TAP&#x2F;TUN设备发送数据。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/types.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/socket.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;linux/if_tun.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;linux/if.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">create_tun</span><span class="params">(<span class="keyword">char</span> *devname,<span class="keyword">int</span> flags)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> fd;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ifreq</span> <span class="title">ifr</span>;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>((fd = open(<span class="string">&quot;/dev/net/tun&quot;</span>,O_RDWR))&lt;<span class="number">0</span>) &#123;</span><br><span class="line">        perror(<span class="string">&quot;open&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">memset</span>(&amp;ifr,<span class="number">0</span>,<span class="keyword">sizeof</span>(ifr));</span><br><span class="line">    <span class="comment">// IFF_NO_PI，package info，tun/tap会默认在网卡包包信息，信息内容如下</span></span><br><span class="line">    <span class="comment">// struct tun_pi &#123;</span></span><br><span class="line">    <span class="comment">//     unsigned short flags;</span></span><br><span class="line">    <span class="comment">//     unsigned short proto;</span></span><br><span class="line">    <span class="comment">// &#125;;</span></span><br><span class="line">    ifr.ifr_flags = flags;</span><br><span class="line">    <span class="built_in">strcpy</span>(ifr.ifr_name,devname); </span><br><span class="line">    <span class="keyword">if</span>(ioctl(fd,TUNSETIFF,(<span class="keyword">void</span>*)&amp;ifr)&lt;<span class="number">0</span>) &#123;</span><br><span class="line">        perror(<span class="string">&quot;ioctl&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> ERROR;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> fd;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="/2022/02/03/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87/tun_tap%E4%BC%A0%E8%BE%93.jpg"></p>
<h2 id="Linux-Bridge"><a href="#Linux-Bridge" class="headerlink" title="Linux Bridge"></a>Linux Bridge</h2><p>Linux Bridge（网桥）是工作在二层的虚拟网络设备，功能类似于物理的交换机。</p>
<p>对于普通的网络设备来说，只有两端，从一端进来的数据会从另一端出去，如物理网卡从外面物理<br>网络收到的数据会转发给内核协议栈，而从内核协议栈过来的数据会转发到外面的物理网络中。而<br>Bridge 不同，Bridge 有多个端口，数据可以从任何端口进来，进来之后从哪个端口出去要看MAC地址，<br>和物理交换机的原理类似。</p>
<p>Bridge有以下特定：</p>
<ul>
<li>Bridge是二层设备，仅用来处理二层的通讯</li>
<li>Bridge使用mac地址表来决定怎么转发帧</li>
<li>Bridge会从host之间的数据通讯包中学习MAC地址，Bridge能做二层转发的原因。</li>
<li>可以是硬件或者纯软件实现（纯软件实现会降低性能，纯用cpu实现）</li>
</ul>
<blockquote>
<p>注：Bridge可能出现二层广播风暴，可以通过开启STP来防止出现环路。</p>
</blockquote>
<h3 id="二层广播风暴"><a href="#二层广播风暴" class="headerlink" title="二层广播风暴"></a>二层广播风暴</h3><h4 id="二层交换机转发机制（bridge）"><a href="#二层交换机转发机制（bridge）" class="headerlink" title="二层交换机转发机制（bridge）"></a>二层交换机转发机制（bridge）</h4><p>交换机对于从一个port N 上incoming frame，学习其source mac x，生成mac address table如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mac x &lt;-----&gt; port n</span><br></pre></td></tr></table></figure>

<p>这样就生成了mac地址和port的映射表，如果交换机收到一个frame，就会查询frame的destination mac和mac address table进行匹配，匹配到了就从对应的port发去。</p>
<p>如果没有匹配到，就认为是 unknown Unicast 或 broadcast，没有办法只好把它从所有Port( 除了接收到Frame的接口）发送出去，到了另外一台交换机也是这么处理，于是这个Frame就一直在封闭的环路里无限的循环。</p>
<p>网桥处理包遵循以下几条原则：</p>
<ol>
<li><p>在一个接口上接收的包不会再在那个接口上发送这个数据包；</p>
</li>
<li><p>每个接收到的数据包都要学习其源地址；</p>
</li>
<li><p>如果数据包是多播或广播包，则要在同一个网段中除了接收端口外的其他所有端口发送这个数据包，如果上层协议栈对多播包感兴趣，则需要把数据包提交给上层协议栈；</p>
</li>
<li><p>如果数据包的目的MAC地址不能再CAM表中找到，则要在同一个网段中除了接收端口外的其他所有端口发送这个数据包；</p>
</li>
<li><p>如果能够在CAM表中查询到目的MAC地址，则在特定的端口上发送这个数据包，如果发送端口和接收端口是同一端口则不发送。</p>
</li>
</ol>
<h4 id="STP协议"><a href="#STP协议" class="headerlink" title="STP协议"></a>STP协议</h4><p>STP（Spanning Tree Protocol）生成树协议，是运行在交换机上的二层破环协议，环路会导致广播风暴、MAC地址表震荡(交换机在学习mac地址时产生广播风暴导致的)等后果，STP的主要目的就是确保在网络中存在冗余路径时，不会产生环路。</p>
<p>通过生成树的选举机制，将优先级高的交换机节点选为根节点，至于从根到下游端口是畅通还是阻断，取决于到根的路径成本cost，谁更接近根，谁就畅通。</p>
<h3 id="Bridge搭建虚拟化环境"><a href="#Bridge搭建虚拟化环境" class="headerlink" title="Bridge搭建虚拟化环境"></a>Bridge搭建虚拟化环境</h3><p>传统虚拟机通过<code>Bridge</code>和<code>tun/tap</code>、<code>veth-pair</code>实现虚拟机的组网。</p>
<p><img src="/2022/02/03/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87/bridge_veth.jpg"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">  +-------------------------------------+------------------------------+----------------------------+</span><br><span class="line">  |                                     |                              |                            |</span><br><span class="line">  |                HOST                 |               VM             |             VM             |</span><br><span class="line">  |                                     |                              |                            |</span><br><span class="line">  |    +--------------------------+     |  +------------------------+  | +------------------------+ |</span><br><span class="line">  |    |  NETWORK PROTOCOL STACK  |     |  | NETWORK PROTOCOL STACK |  | | NETWORK PROTOCOL STACK | |</span><br><span class="line">  |    +--------------------------+     |  +------------------------+  | +------------------------+ |</span><br><span class="line">  +-------------------------------------------------------------------------------------------------+</span><br><span class="line">  |                                     |                              |                            |</span><br><span class="line">  |                                     |                              |                            |</span><br><span class="line">  |             +---------+             |          +---------+         |         +---------+        |</span><br><span class="line">  |             |    .1   |             |          |   .2    |         |         |    .3   |        |</span><br><span class="line">  | +-----+     +---------+     +-----+ |          +---------+         |         +---------+        |</span><br><span class="line">  | | eth0| &lt;-&gt; |   br0   | &lt;-&gt; | tap | |          |   eth0  |         |         |   eth0  |        |</span><br><span class="line">  | +--+--+     +---------+     +--+--+ |          +----+----+         |         +----+----+        |</span><br><span class="line">  |    |                           ^    |               ^              |              ^             |</span><br><span class="line">  |    |             ^             |    |               |              |              |             |</span><br><span class="line">  |    |             |             +--------------------+              |              |             |</span><br><span class="line">  |    |             v                  |                              |              |             |</span><br><span class="line">  |    |                                |                              |              |             |</span><br><span class="line">  |    |        +---------+             |                              |              |             |</span><br><span class="line">  |    |        |   tap   | &lt;---------------------------------------------------------+             |</span><br><span class="line">  |    |        +---------+             |                              |                            |</span><br><span class="line">  +-------------------------------------+------------------------------+----------------------------+</span><br><span class="line">       |</span><br><span class="line">       v</span><br><span class="line">Physical Network</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p><img src="/2022/02/03/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87/bridge_tap.jpg"></p>
<h3 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h3><ul>
<li>当在同一台宿主机上需要连接多个虚拟机或容器时。</li>
<li>对于拥有多个网桥的混合环境。</li>
<li>需要应用高级流量控制，FDB的维护。</li>
</ul>
<h2 id="MACVLAN"><a href="#MACVLAN" class="headerlink" title="MACVLAN"></a>MACVLAN</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>Macvlan 允许你在主机的一个网络接口上配置多个虚拟的网络接口，这些网络 interface 有自己独立的 MAC 地址，也可以配置上 IP 地址进行通信。Macvlan 下的虚拟机或者容器网络和主机在同一个网段中，共享同一个广播域。</p>
<h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>同一 VLAN 间数据传输是通过二层互访，即 MAC 地址实现的，不需要使用路由。不同 VLAN 的用户单播默认不能直接通信，如果想要通信，还需要三层设备做路由，Macvlan 也是如此。用 Macvlan 技术虚拟出来的虚拟网卡，在逻辑上和物理网卡是对等的。物理网卡也就相当于一个交换机，记录着对应的虚拟网卡和 MAC 地址，当物理网卡收到数据包后，会根据目的 MAC 地址判断这个包属于哪一个虚拟网卡。这也就意味着，只要是从 Macvlan 子接口发来的数据包（或者是发往 Macvlan 子接口的数据包），物理网卡只接收数据包，不处理数据包。</p>
<blockquote>
<p>macvlan的子接口无法ping通自己的父接口</p>
</blockquote>
<p><img src="/2022/02/03/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87/macvlan_theory.jpg"></p>
<h3 id="macvlan特点"><a href="#macvlan特点" class="headerlink" title="macvlan特点"></a>macvlan特点</h3><ol>
<li>一张实体网卡上设定多个MAC地址</li>
<li>带有上述设定的mac地址的网卡称为子接口；而实体网卡则称为父接口</li>
<li>父接口可以是一个物理接口，也可以是一个802.1q（vlan）的子接口，也可以是bonding接口。</li>
<li>子接口无法直接与父接口通讯</li>
<li>若需要子接口和主机通信，则需要额外建立一个子接口提供给主机使用。</li>
</ol>
<h3 id="macvlan的工作模式"><a href="#macvlan的工作模式" class="headerlink" title="macvlan的工作模式"></a>macvlan的工作模式</h3><h4 id="vepa-virtual-ethernet-port-aggregator"><a href="#vepa-virtual-ethernet-port-aggregator" class="headerlink" title="vepa(virtual ethernet port aggregator)"></a>vepa(virtual ethernet port aggregator)</h4><p><img src="/2022/02/03/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87/macvlan_vepa.jpg"><br>在 VEPA 模式下，所有从 Macvlan 接口发出的流量，不管目的地全部都发送给父接口，即使流量的目的地是共享同一个父接口的其它 Macvlan 接口。在二层网络场景下，由于生成树协议的原因，两个 Macvlan 接口之间的通讯会被阻塞，这时需要上层路由器上为其添加路由（需要外部交换机配置 Hairpin 支持，即需要兼容 802.1Qbg 的交换机支持，其可以把源和目的地址都是本地 Macvlan 接口地址的流量发回给相应的接口）。此模式下从父接口收到的广播包，会泛洪给 VEPA 模式的所有子接口。</p>
<blockquote>
<p>在这种模式下交换机必须要支持hairpin协议才能使得交换机实现源地址和目的地址都是本地macvlan地址的包发给响应接口。<br>由于现有交换机设备通常不支持hairpin协议，因此可以在本地通过bridge实现hairpin协议，将流量从一个子接口转发到另一个子接口。</p>
</blockquote>
<p><img src="/2022/02/03/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87/macvlan_vepa_bridge.jpg"></p>
<blockquote>
<p>VEPA 和 Passthru 模式下，两个 Macvlan 接口之间的通信会经过主接口两次：第一次是发出的时候，第二次是返回的时候。这样会影响物理接口的宽带，也限制了不同 Macvlan 接口之间通信的速度。如果多个 Macvlan 接口之间通信比较频繁，对于性能的影响会比较明显。</p>
</blockquote>
<h4 id="bridge"><a href="#bridge" class="headerlink" title="bridge"></a>bridge</h4><p><img src="/2022/02/03/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87/macvlan_bridge.jpg"><br>此种模式类似 Linux 的 Bridge，拥有相同父接口的两块 Macvlan 虚拟网卡是可以直接通讯的，不需要把流量通过父网卡发送到外部网络，广播帧将会被泛洪到连接在”网桥”上的所有其他子接口和物理接口。这比较适用于让共享同一个父接口的 Macvlan 网卡进行直接通讯的场景。</p>
<blockquote>
<p>Bridge 模式有个缺点：如果父接口 down 掉，所有的 Macvlan 子接口也会全部 down 掉，同时子接口之间也将无法进行通讯。</p>
</blockquote>
<h4 id="private"><a href="#private" class="headerlink" title="private"></a>private</h4><p><img src="/2022/02/03/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87/macvlan_private.jpg"></p>
<p>此种模式相当于 VEPA 模式的增强模式，其完全阻止共享同一父接口的 Macvlan 虚拟网卡之间的通讯，即使配置了 Hairpin 让从父接口发出的流量返回到宿主机，相应的通讯流量依然被丢弃。具体实现方式是丢弃广播&#x2F;多播数据，这就意味着以太网地址解析 arp 将不可运行，除非手工探测 MAC 地址，否则通信将无法在同一宿主机下的多个 Macvlan 网卡间展开。</p>
<h4 id="passthru"><a href="#passthru" class="headerlink" title="passthru"></a>passthru</h4><p><img src="/2022/02/03/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87/macvlan_passthru.jpg"></p>
<p>此种模式会直接把父接口和相应的MacVLAN接口捆绑在一起，这种模式每个父接口只能和一个 Macvlan 虚拟网卡接口进行捆绑，并且 Macvlan 虚拟网卡接口继承父接口的 MAC 地址。</p>
<h3 id="使用场景-1"><a href="#使用场景-1" class="headerlink" title="使用场景"></a>使用场景</h3><ul>
<li>仅仅需要为虚拟机或容器提供访问外部物理网络的连接。</li>
<li>Macvlan 占用较少的 CPU，同时提供较高的吞吐量。</li>
<li>当使用 Macvlan 时，宿主机无法和 VM 或容器直接进行通讯。</li>
</ul>
<h3 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h3><p>部分NIC和无限网卡上对于MAC地址有相应的限制。</p>
<h3 id="验证测试"><a href="#验证测试" class="headerlink" title="验证测试"></a>验证测试</h3><h4 id="测试环境"><a href="#测试环境" class="headerlink" title="测试环境"></a>测试环境</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@giddypoet ~]# cat /etc/system-release</span><br><span class="line">CentOS Linux release 7.4.1708 (Core) </span><br><span class="line">[root@giddypoet ~]# uname -a</span><br><span class="line">Linux giddypoet 3.10.0-693.el7.x86_64 #1 SMP Tue Aug 22 21:09:27 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux</span><br></pre></td></tr></table></figure>

<h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[root@giddypoet ~]# modprobe macvlan</span><br><span class="line">[root@giddypoet ~]# lsmod |grep macvlan</span><br><span class="line">macvlan                19239  0 </span><br><span class="line">[root@giddypoet ~]# ip link add link ens39 mac0 type macvlan mode bridge</span><br><span class="line">[root@giddypoet ~]# ip link add link ens39 mac1 type macvlan mode bridge</span><br><span class="line">[root@giddypoet ~]# ip -d link show mac1 </span><br><span class="line">14: mac1@ens39: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT qlen 1000</span><br><span class="line">    link/ether 3a:ce:df:da:3d:8c brd ff:ff:ff:ff:ff:ff promiscuity 0 </span><br><span class="line">    macvlan  mode bridge addrgenmode eui64 </span><br><span class="line">[root@giddypoet ~]# ip -d link show mac0 </span><br><span class="line">13: mac0@ens39: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT qlen 1000</span><br><span class="line">    link/ether 72:e1:da:bd:3f:82 brd ff:ff:ff:ff:ff:ff promiscuity 0 </span><br><span class="line">    macvlan  mode bridge addrgenmode eui64 </span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建netns，由于内核规则通常会存在禁止内部转发以ip_forward相关策略，通常通过netns来验证转发功能，以减少对内核配置的修改</span></span><br><span class="line">[root@giddypoet ~]# ip netns add ns0</span><br><span class="line">[root@giddypoet ~]# ip netns add ns1</span><br><span class="line">[root@giddypoet ~]# ip link set mac0 netns ns0</span><br><span class="line">[root@giddypoet ~]# ip link set mac1 netns ns1</span><br><span class="line">[root@giddypoet ~]# ip netns exec ns0 bash</span><br><span class="line">[root@giddypoet ~]# ip a a 10.92.3.2/24 dev mac0</span><br><span class="line">[root@giddypoet ~]# ip link set mac0 up</span><br><span class="line">[root@giddypoet ~]# exit</span><br><span class="line">exit</span><br><span class="line">[root@giddypoet ~]# ip netns exec ns1 bash</span><br><span class="line">[root@giddypoet ~]# ip a a 10.92.3.3/24 dev mac1</span><br><span class="line">[root@giddypoet ~]# ip link set mac1 up</span><br></pre></td></tr></table></figure>

<p>上述准备完成后，可以发现通过macvlan能够ping通对端，如果需要连接网络，则需要在ns0和ns1上配置默认路由，后面相应的步骤就不做展示了。</p>
<blockquote>
<p>通过在父接口上抓包能够抓到ping包</p>
</blockquote>
<h2 id="MACVTAP"><a href="#MACVTAP" class="headerlink" title="MACVTAP"></a>MACVTAP</h2><p>传统使用tap+bridge实现网络虚拟化的技术存在以下缺点：</p>
<ol>
<li>每台宿主机内都存在Bridge会使网络拓扑变得复杂，相当于增加了交换机的级联层数</li>
<li>同一宿主机上虚拟机之间的流量直接在Bridge完成交换，使流量监控、监管变得困难。</li>
<li>Bridge是软件实现的二层交换技术，会增加服务器的负担。</li>
</ol>
<p>其实技术上的演进路线如下：<br>tap+bridge -&gt; macvlan -&gt; macvtap</p>
<p>技术标准主要有两个：<br>802.1qbg 和 802.1qbh(这个是思科提出的标准，思科还在此标准上进行了多次演进)。</p>
<p>和TAP设备一样，每一个MACVTAP设备都拥有一个对应和Linux字符设备，因此能直接KVM&#x2F;QEMU使用，方便完成网络数据交换工作。</p>
<p>实际上mactap只是将收到的流量转发给&#x2F;dev&#x2F;net&#x2F;tap，直接和用户态交互，虚拟机做相关操作。</p>
<p><img src="/2022/02/03/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87/mactap.jpg"></p>
<h3 id="工作模式"><a href="#工作模式" class="headerlink" title="工作模式"></a>工作模式</h3><p>其实这三种工作模式都是继承于MACVLAN的工作模式。</p>
<ul>
<li>VEPA</li>
<li>BRIDGE</li>
<li>PRIVATE</li>
</ul>
<h3 id="测试验证"><a href="#测试验证" class="headerlink" title="测试验证"></a>测试验证</h3><p>测试环境基于centos7.4，与上述macvlan测试环境一致。</p>
<h4 id="测试-1"><a href="#测试-1" class="headerlink" title="测试"></a>测试</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> macvtap依赖于macvlan的驱动，从这里也可以看出macvtap继承于macvlan</span></span><br><span class="line">[root@giddypoet ~]# modprobe macvtap</span><br><span class="line">[root@giddypoet ~]# lsmod |grep macvtap</span><br><span class="line">macvtap                22497  0 </span><br><span class="line">macvlan                19239  1 macvtap</span><br><span class="line">[root@giddypoet ~]# rmmod macvlan</span><br><span class="line">rmmod: ERROR: Module macvlan is in use by: macvtap</span><br><span class="line">[root@giddypoet ~]# ip link add link ens39 tap0 type macvtap mode bridge</span><br><span class="line">[root@giddypoet ~]# ip link add link ens39 tap1 type macvtap mode bridge</span><br><span class="line">[root@giddypoet ~]# ip -d link show tap0</span><br><span class="line">15: tap0@ens39: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT qlen 500</span><br><span class="line">    link/ether 3a:56:f6:88:05:01 brd ff:ff:ff:ff:ff:ff promiscuity 0 </span><br><span class="line">    macvtap  mode bridge addrgenmode eui64 </span><br><span class="line">[root@giddypoet ~]# ip -d link show tap1</span><br><span class="line">16: tap1@ens39: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT qlen 500</span><br><span class="line">    link/ether 76:b2:9f:9e:1f:9a brd ff:ff:ff:ff:ff:ff promiscuity 0 </span><br><span class="line">    macvtap  mode bridge addrgenmode eui64 </span><br><span class="line">[root@giddypoet ~]# ip link set tap0 netns ns0</span><br><span class="line">[root@giddypoet ~]# ip link set tap1 netns ns1</span><br><span class="line">[root@giddypoet ~]# ip link set tap0 netns ns0</span><br><span class="line">[root@giddypoet ~]# ip link set tap1 netns ns1</span><br><span class="line">[root@giddypoet ~]# ip netns exec ns0 bash</span><br><span class="line">[root@giddypoet ~]# ip a a 192.168.3.2/24 dev tap0</span><br><span class="line">[root@giddypoet ~]# ip link set tap0 up</span><br><span class="line">[root@giddypoet ~]# exit</span><br><span class="line">exit</span><br><span class="line">[root@giddypoet ~]# ip netns exec ns1 bash</span><br><span class="line">[root@giddypoet ~]# ip a a 192.168.3.3/24 dev tap1</span><br><span class="line">[root@giddypoet ~]# ip link set tap1 up</span><br></pre></td></tr></table></figure>

<blockquote>
<p>完成上述配置后，macvtap两两就可以ping通了</p>
</blockquote>
<h2 id="IPVLAN"><a href="#IPVLAN" class="headerlink" title="IPVLAN"></a>IPVLAN</h2><p>IPVLAN其实原理上和MACVLAN一致，唯一区别在于IPVLAN虚拟出来的子接口和父接口mac地址一致。</p>
<p><img src="/2022/02/03/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87/ipvlan.jpg"></p>
<h3 id="工作模式-1"><a href="#工作模式-1" class="headerlink" title="工作模式"></a>工作模式</h3><h4 id="L2"><a href="#L2" class="headerlink" title="L2"></a>L2</h4><p>和macvlan一样，父接口更像一个网桥和二层交换机。</p>
<p><img src="/2022/02/03/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87/ipvlan_l2.jpg"></p>
<h4 id="L3"><a href="#L3" class="headerlink" title="L3"></a>L3</h4><p>在l3模式下，父接口像一个三层交换机一样，用于转发流量，提供了更强的伸缩性。</p>
<p><img src="/2022/02/03/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87/ipvlan_l3.jpg"></p>
<h3 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h3><p>其实个人觉得，大部分ipvlan的场景和macvlan相似，核心在于ipvlan共用父接口的mac地址，在特殊的场景下有着特殊的应用价值：</p>
<ol>
<li>设备对网卡上的mac地址有数量限制时</li>
<li>在802.11协议下（无线协议）对于终端的mac地址有要求</li>
<li>大量mac地址对性能会对主机产生影响</li>
</ol>
<h3 id="测试-2"><a href="#测试-2" class="headerlink" title="测试"></a>测试</h3><p>由于试用的linux内核版本目前不支持ipvlan，同时基本原理和macvlan一致，暂时未对其做相关测试。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>其实linux上还存在其他虚拟设备如：</p>
<ul>
<li>bond</li>
<li>vcan</li>
<li>vxcan</li>
<li>……</li>
</ul>
<p>上述设备的用途通常不在于云原生场景中，后续再对其他虚拟设备进行介绍。</p>

	
	</div>
  <a type="button" href="/2022/02/03/虚拟网络设备/#more" class="btn btn-default more">Read More</a>
</div>

		
			
	
	<!-- display as entry -->
<div class="row">
	<div class="col-md-8">
		<h3 class="title">
			<a href="/2022/02/03/nic收发包/" >nic网卡收包</a>
		</h3>
		</div>
	<div class="col-md-4">
		<div class="date">post @ 2022-02-03  </div>
		</div>
	</div>
	


			<div class="entry">
  <div class="row">
	
	
		<h2 id="网卡收发包"><a href="#网卡收发包" class="headerlink" title="网卡收发包"></a>网卡收发包</h2><h3 id="程序控制I-x2F-O"><a href="#程序控制I-x2F-O" class="headerlink" title="程序控制I&#x2F;O"></a>程序控制I&#x2F;O</h3><p>这是最简单的一种 I&#x2F;O 模式，也叫忙等待或者轮询：用户通过发起一个系统调用，陷入内核态，内核将系统调用翻译成一个对应设备驱动程序的过程调用，接着设备驱动程序会启动 I&#x2F;O 不断循环去检查该设备，看看是否已经就绪，一般通过返回码来表示，I&#x2F;O 结束之后，设备驱动程序会把数据送到指定的地方并返回，切回用户态。</p>
<blockquote>
<p>cpu一直处于忙轮询状态，调用驱动程序检查设备状态</p>
</blockquote>
<p><img src="/2022/02/03/nic%E6%94%B6%E5%8F%91%E5%8C%85/roundrobin_io.png"></p>
<h3 id="中断I-x2F-O"><a href="#中断I-x2F-O" class="headerlink" title="中断I&#x2F;O"></a>中断I&#x2F;O</h3><ol>
<li>用户进程发起一个 read() 系统调用读取磁盘文件，陷入内核态并由其所在的 CPU 通过设备驱动程序向设备寄存器写入一个通知信号，告知设备控制器 (我们这里是磁盘控制器)要读取数据；</li>
<li>磁盘控制器启动磁盘读取的过程，把数据从磁盘拷贝到磁盘控制器缓冲区里；</li>
<li>完成拷贝之后磁盘控制器会通过总线发送一个中断信号到中断控制器，如果此时中断控制器手头还有正在处理的中断或者有一个和该中断信号同时到达的更高优先级的中断，则这个中断信号将被忽略，而磁盘控制器会在后面持续发送中断信号直至中断控制器受理；</li>
<li>中断控制器收到磁盘控制器的中断信号之后会通过地址总线存入一个磁盘设备的编号，表示这次中断需要关注的设备是磁盘；</li>
<li>中断控制器向 CPU 置起一个磁盘中断信号；</li>
<li>CPU 收到中断信号之后停止当前的工作，把当前的 PC&#x2F;PSW 等寄存器压入堆栈保存现场，然后从地址总线取出设备编号，通过编号找到中断向量所包含的中断服务的入口地址，压入 PC 寄存器，开始运行磁盘中断服务，把数据从磁盘控制器的缓冲区拷贝到主存里的内核缓冲区；</li>
<li>最后 CPU 再把数据从内核缓冲区拷贝到用户缓冲区，完成读取操作，read() 返回，切换回用户态。</li>
</ol>
<blockquote>
<p>内核调用硬盘驱动，实现将数据从硬盘拷贝至硬盘控制器缓冲区，然后传递设备号，触发中断（硬中断），之后cpu将数据从硬盘控制器缓存区拷贝出来传递到内存中，再拷贝到用户缓存区。</p>
</blockquote>
<p><img src="/2022/02/03/nic%E6%94%B6%E5%8F%91%E5%8C%85/irq_io.png"></p>
<p><img src="/2022/02/03/nic%E6%94%B6%E5%8F%91%E5%8C%85/irq_io.webp"></p>
<h3 id="DMA-I-x2F-O"><a href="#DMA-I-x2F-O" class="headerlink" title="DMA I&#x2F;O"></a>DMA I&#x2F;O</h3><p>在上述中断I&#x2F;O中，当网卡控制器将数据从硬盘上存储到自身的缓存区后，cpu负责将数据从网卡控制器的缓存区搬运至内存中再拷贝至用户态，在这两次数据拷贝阶段中CPU是完全被占用而不能处理其他工作的。由于从内核态拷贝到用户态都在主存中，只能由cpu完成，但是第 6 步的数据拷贝，是从磁盘控制器的缓冲区到主存，是两个设备之间的数据传输，这一步并非一定要 CPU 来完成，可以借助 DMA 来完成，减轻 CPU 的负担。</p>
<p>DMA 全称是 Direct Memory Access，也即直接存储器存取，是一种用来提供在外设和存储器之间或者存储器和存储器之间的高速数据传输。整个过程无须 CPU 参与，数据直接通过 DMA 控制器进行快速地移动拷贝，节省 CPU 的资源去做其他工作。</p>
<ol>
<li>用户进程发起一个 read() 系统调用读取磁盘文件，陷入内核态并由其所在的 CPU 通过设置 DMA 控制器的寄存器对它进行编程：把内核缓冲区和磁盘文件的地址分别写入 MAR 和 ADR 寄存器，然后把期望读取的字节数写入 WC 寄存器，启动 DMA 控制器；</li>
<li>DMA 控制器根据 ADR 寄存器里的信息知道这次 I&#x2F;O 需要读取的外设是磁盘的某个地址，便向磁盘控制器发出一个命令，通知它从磁盘读取数据到其内部的缓冲区里；</li>
<li>磁盘控制器启动磁盘读取的过程，把数据从磁盘拷贝到磁盘控制器缓冲区里，并对缓冲区内数据的校验和进行检验，如果数据是有效的，那么 DMA 就可以开始了；</li>
<li>DMA 控制器通过总线向磁盘控制器发出一个读请求信号从而发起 DMA 传输，这个信号和前面的中断驱动 I&#x2F;O 小节里 CPU 发给磁盘控制器的读请求是一样的，它并不知道或者并不关心这个读请求是来自 CPU 还是 DMA 控制器；</li>
<li>紧接着 DMA 控制器将引导磁盘控制器将数据传输到 MAR 寄存器里的地址，也就是内核缓冲区；</li>
<li>数据传输完成之后，返回一个 ack 给 DMA 控制器，WC 寄存器里的值会减去相应的数据长度，如果 WC 还不为 0，则重复第 4 步到第 6 步，一直到 WC 里的字节数等于 0；</li>
<li>收到 ack 信号的 DMA 控制器会通过总线发送一个中断信号到中断控制器，如果此时中断控制器手头还有正在处理的中断或者有一个和该中断信号同时到达的更高优先级的中断，则这个中断信号将被忽略，而 DMA 控制器会在后面持续发送中断信号直至中断控制器受理；</li>
<li>中断控制器收到磁盘控制器的中断信号之后会通过地址总线存入一个主存设备的编号，表示这次中断需要关注的设备是主存；</li>
<li>中断控制器向 CPU 置起一个 DMA 中断的信号；</li>
<li>CPU 收到中断信号之后停止当前的工作，把当前的 PC&#x2F;PSW 等寄存器压入堆栈保存现场，然后从地址总线取出设备编号，通过编号找到中断向量所包含的中断服务的入口地址，压入 PC 寄存器，开始运行 DMA 中断服务，把数据从内核缓冲区拷贝到用户缓冲区，完成读取操作，read() 返回，切换回用户态。</li>
</ol>
<blockquote>
<p>简单来说就是通过DMA将数据从硬盘控制器缓存区拷贝到内存中，减少了一次CPU COPY。</p>
</blockquote>
<p><img src="/2022/02/03/nic%E6%94%B6%E5%8F%91%E5%8C%85/dma_io.png"></p>
<p><img src="/2022/02/03/nic%E6%94%B6%E5%8F%91%E5%8C%85/dma_io.webp"></p>
<h3 id="napi-I-x2F-O"><a href="#napi-I-x2F-O" class="headerlink" title="napi I&#x2F;O"></a>napi I&#x2F;O</h3><p>napi并不是基于硬件实现的，实际上napi是一种新的linux网卡数据处理API，简单来说其杂合中断和轮询的技术。</p>
<p>中断的好处是响应及时，如果数据量较小，则不会占用太多的CPU时间；缺点是数据量大时，会产生过多中断，频繁上下文切换，同时中断也会大量消耗大量的CPU时间。</p>
<p>轮询方式与中断方式相反，它更适合处理大量数据，因为每次轮询不需要消耗过多的CPU时间；缺点是即使只接收很少数据或不接收数据时，也要占用CPU时间。</p>
<p>实际上就是通过驱动本身的poll函数，实现一次中断收多个包。</p>
<h3 id="硬中断和软中断"><a href="#硬中断和软中断" class="headerlink" title="硬中断和软中断"></a>硬中断和软中断</h3><p>NAPI：数据包到来，第一个数据包产生硬件中断，中断处理程序将设备的napi_struct结构挂在当前cpu的待收包设备链表softnet_data-&gt;poll_list中，并触发软中断，软中断执行过程中，遍历softnet_data-&gt;poll_list中的所有设备，依次调用其收包函数napi_sturct-&gt;poll，处理收包过程；</p>
<p>非NAPI：每个数据包到来，都会产生硬件中断，中断处理程序将收到的包放入当前cpu的收包队列softnet_data-&gt;input_pkt_queue中，并且将非napi设备对应的虚拟设备napi结构softnet-&gt;backlog结构挂在当前cpu的待收包设备链表softnet_data-&gt;poll_list中，并触发软中断，软中断处理过程中，会调用backlog的回调处理函数process_backlog，将收包队列input_pkt_queue合并到softdata-&gt;process_queue后面，并依次处理该队列中的数据包；</p>

	
	</div>
  <a type="button" href="/2022/02/03/nic收发包/#more" class="btn btn-default more">Read More</a>
</div>

		

		</div>

		<!-- pagination -->
		<div>
  		<center>
		<div class="pagination">

   
</div>

  		</center>
		</div>

		
		
	</div> <!-- col-md-9 -->

	
		<div class="col-md-3">
	<div id="sidebar">
	
			
  <div id="site_search">
   <div class="form-group">
    <input type="text" id="local-search-input" name="q" results="0" placeholder="Search" class="st-search-input st-default-search-input form-control"/>
   </div>  
  <div id="local-search-result"></div>
  </div>


		
			
	<div class="widget">
		<h4>Categories</h4>
		<ul class="tag_box inline list-unstyled">
		
			<li><a href="/categories/os/">os<span>5</span></a></li>
		
		</ul>
	</div>

		
			
	<div class="widget">
		<h4>Tag Cloud</h4>
		<ul class="tag_box inline list-unstyled">		
		
			<li><a href="/tags/network/">network<span>4</span></a></li>
		
			<li><a href="/tags/dpdk/">dpdk<span>1</span></a></li>
		
			<li><a href="/tags/memory/">memory<span>1</span></a></li>
		
		 
		</ul>
	</div>


		
			
<div class="widget">
  <h4>Recent Posts</h4>
  <ul class="entry list-unstyled">
    
      <li>
        <a href="/2022/02/08/linux网卡聚合及bond模式原理/" ><i class="fa fa-file-o"></i>linux网卡聚合及bond模式原理</a>
      </li>
    
      <li>
        <a href="/2022/02/03/一次内存异常问题排查经历/" ><i class="fa fa-file-o"></i>一次内存异常问题排查经历</a>
      </li>
    
      <li>
        <a href="/2022/02/03/dpdk安装部署/" ><i class="fa fa-file-o"></i>dpdk安装部署</a>
      </li>
    
      <li>
        <a href="/2022/02/03/虚拟网络设备/" ><i class="fa fa-file-o"></i>虚拟网络设备</a>
      </li>
    
      <li>
        <a href="/2022/02/03/nic收发包/" ><i class="fa fa-file-o"></i>nic网卡收包</a>
      </li>
    
  </ul>
</div>

		
			
<div class="widget">
	<h4>Links</h4>
	<ul class="blogroll list-unstyled">
	
		<li><i class="fa fa-github"></i><a href="https://github.com/wzpan/hexo-theme-freemind" title="Freemind's Github repository." target="_blank">Freemind</a></li>
	
		<li><i class="fa fa-github"></i><a href="https://github.com/kristopolous/BOOTSTRA.386" title="BOOTSTRA.386's Github repository." target="_blank">BOOTSTRA.386</a></li>
	
		<li><i class="fa fa-github"></i><a href="https://github.com/blackshow/hexo-theme-freemind.386" title="Freemind.386's Github repository." target="_blank">Freemind.386</a></li>
	
		<li><i class="fa fa-github"></i><a href="http://www.github.com/GiddyPoet" title="My Github account." target="_blank">My Github</a></li>
	
	</ul>
</div>


		
	</div> <!-- sidebar -->
</div> <!-- col-md-3 -->

	
	
</div> <!-- row-fluid -->
	</div>
  </div>
  <div class="container-narrow">
  <footer> <p>
  &copy; 2022 GiddyPoet
  
      with help from <a href="http://hexo.io/" target="_blank">Hexo</a>,<a target="_blank" rel="noopener" href="http://github.com/wzpan/hexo-theme-freemind/">Freemind</a>,<a href="http://getbootstrap.com/" target="_blank">Twitter Bootstrap</a> and <a href="http://getbootstrap.com/" target="_blank">BOOTSTRA.386</a>. 
     <br> Theme by <a target="_blank" rel="noopener" href="http://github.com/wzpan/hexo-theme-freemind/">Freemind.386</a>.    
</p>
 </footer>
</div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>⬆︎TOP</span>
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>
<script src="/js/search.js"></script> 


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>



</body>
   </html>
